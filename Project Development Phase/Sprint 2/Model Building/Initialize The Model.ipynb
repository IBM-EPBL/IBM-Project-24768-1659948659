{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63343221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494748ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=ImageDataGenerator(rescale=1./255,zoom_range=0.2,horizontal_flip=True)\n",
    "test_data=ImageDataGenerator(rescale=1./255,validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4f4f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2515 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "xtrain=train_data.flow_from_directory('E:/Projects/Jyupter/Dataset/Hand Sign/Train',\n",
    "                                      target_size=(64,64),\n",
    "                                      class_mode='categorical',\n",
    "                                      batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b243c950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2515 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "xtest=test_data.flow_from_directory('E:/Projects/Jyupter/Dataset/Hand Sign/Test',\n",
    "                                         target_size=(64,64),\n",
    "                                         class_mode='categorical',\n",
    "                                         batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae86824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1615472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Convolution2D(32,(3,3),activation='relu',input_shape=(64,64,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(300,activation='relu'))\n",
    "model.add(Dense(150,activation='relu'))\n",
    "model.add(Dense(36,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b71e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e99e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15039d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd7bfc43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "26/26 [==============================] - 13s 521ms/step - loss: 0.0995 - accuracy: 0.9670 - val_loss: 0.0390 - val_accuracy: 0.9881\n",
      "Epoch 2/25\n",
      "26/26 [==============================] - 13s 527ms/step - loss: 0.0785 - accuracy: 0.9769 - val_loss: 0.0432 - val_accuracy: 0.9829\n",
      "Epoch 3/25\n",
      "26/26 [==============================] - 14s 535ms/step - loss: 0.0811 - accuracy: 0.9742 - val_loss: 0.0474 - val_accuracy: 0.9853\n",
      "Epoch 4/25\n",
      "26/26 [==============================] - 14s 549ms/step - loss: 0.0872 - accuracy: 0.9706 - val_loss: 0.0373 - val_accuracy: 0.9885\n",
      "Epoch 5/25\n",
      "26/26 [==============================] - 14s 558ms/step - loss: 0.0600 - accuracy: 0.9809 - val_loss: 0.0269 - val_accuracy: 0.9924\n",
      "Epoch 6/25\n",
      "26/26 [==============================] - 15s 570ms/step - loss: 0.0555 - accuracy: 0.9817 - val_loss: 0.0242 - val_accuracy: 0.9917\n",
      "Epoch 7/25\n",
      "26/26 [==============================] - 15s 575ms/step - loss: 0.0529 - accuracy: 0.9825 - val_loss: 0.0613 - val_accuracy: 0.9793\n",
      "Epoch 8/25\n",
      "26/26 [==============================] - 15s 577ms/step - loss: 0.0832 - accuracy: 0.9714 - val_loss: 0.0312 - val_accuracy: 0.9897\n",
      "Epoch 9/25\n",
      "26/26 [==============================] - 15s 573ms/step - loss: 0.0627 - accuracy: 0.9777 - val_loss: 0.0633 - val_accuracy: 0.9769\n",
      "Epoch 10/25\n",
      "26/26 [==============================] - 15s 587ms/step - loss: 0.0643 - accuracy: 0.9801 - val_loss: 0.0161 - val_accuracy: 0.9960\n",
      "Epoch 11/25\n",
      "26/26 [==============================] - 15s 575ms/step - loss: 0.0759 - accuracy: 0.9742 - val_loss: 0.0345 - val_accuracy: 0.9849\n",
      "Epoch 12/25\n",
      "26/26 [==============================] - 15s 569ms/step - loss: 0.0495 - accuracy: 0.9845 - val_loss: 0.0161 - val_accuracy: 0.9964\n",
      "Epoch 13/25\n",
      "26/26 [==============================] - 15s 580ms/step - loss: 0.0646 - accuracy: 0.9793 - val_loss: 0.0188 - val_accuracy: 0.9956\n",
      "Epoch 14/25\n",
      "26/26 [==============================] - 15s 570ms/step - loss: 0.0478 - accuracy: 0.9841 - val_loss: 0.0148 - val_accuracy: 0.9960\n",
      "Epoch 15/25\n",
      "26/26 [==============================] - 15s 572ms/step - loss: 0.0439 - accuracy: 0.9825 - val_loss: 0.0262 - val_accuracy: 0.9913\n",
      "Epoch 16/25\n",
      "26/26 [==============================] - 15s 578ms/step - loss: 0.0389 - accuracy: 0.9857 - val_loss: 0.0107 - val_accuracy: 0.9992\n",
      "Epoch 17/25\n",
      "26/26 [==============================] - 15s 571ms/step - loss: 0.0348 - accuracy: 0.9857 - val_loss: 0.0125 - val_accuracy: 0.9968\n",
      "Epoch 18/25\n",
      "26/26 [==============================] - 15s 574ms/step - loss: 0.0407 - accuracy: 0.9877 - val_loss: 0.0183 - val_accuracy: 0.9940\n",
      "Epoch 19/25\n",
      "26/26 [==============================] - 15s 577ms/step - loss: 0.0600 - accuracy: 0.9797 - val_loss: 0.0207 - val_accuracy: 0.9960\n",
      "Epoch 20/25\n",
      "26/26 [==============================] - 15s 581ms/step - loss: 0.0374 - accuracy: 0.9897 - val_loss: 0.0242 - val_accuracy: 0.9909\n",
      "Epoch 21/25\n",
      "26/26 [==============================] - 15s 607ms/step - loss: 0.0484 - accuracy: 0.9841 - val_loss: 0.0267 - val_accuracy: 0.9913\n",
      "Epoch 22/25\n",
      "26/26 [==============================] - 15s 594ms/step - loss: 0.0356 - accuracy: 0.9893 - val_loss: 0.0105 - val_accuracy: 0.9984\n",
      "Epoch 23/25\n",
      "26/26 [==============================] - 15s 583ms/step - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.0038 - val_accuracy: 0.9996\n",
      "Epoch 24/25\n",
      "26/26 [==============================] - 15s 584ms/step - loss: 0.0278 - accuracy: 0.9924 - val_loss: 0.0080 - val_accuracy: 0.9980\n",
      "Epoch 25/25\n",
      "26/26 [==============================] - 15s 588ms/step - loss: 0.0261 - accuracy: 0.9924 - val_loss: 0.1143 - val_accuracy: 0.9722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x220010024f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain,steps_per_epoch=len(xtrain),\n",
    "          epochs=25,validation_data=xtest,validation_steps=len(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18efaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Hand-SignV2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d7f393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f1141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08f6217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_img='E:/Projects/Jyupter/Dataset/Hand Sign/Test/9/hand1_9_left_seg_4_cropped.jpeg' \n",
    "img=image.load_img(fl_img,target_size=(64,64))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x,axis=0)\n",
    "pred=np.argmax(model.predict(x))\n",
    "print(pred)\n",
    "op=['0','1','2','3','4','5','6','7','8','9','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "op[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0eb5114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_img='E:/Projects/Jyupter/Dataset/conversation engine for deaf and dumb/Dataset/test_set/A/15.png' \n",
    "img=image.load_img(fl_img,target_size=(64,64))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x,axis=0)\n",
    "pred=np.argmax(model.predict(x))\n",
    "print(pred)\n",
    "op=['A','B','C','D','E','F','G','H','I']\n",
    "op[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7d0c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fef39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tensorflow.keras.models.load_model('Hand-SignV2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61a420ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_img='E:/Projects/Jyupter/Dataset/conversation engine for deaf and dumb/Dataset/test_set/A/15.png' \n",
    "img=image.load_img(fl_img,target_size=(64,64))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x,axis=0)\n",
    "pred=np.argmax(new_model.predict(x))\n",
    "print(pred)\n",
    "op=['A','B','C','D','E','F','G','H','I']\n",
    "op[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8a27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_img='E:/Projects/Jyupter/Dataset/Hand Sign/Test/9/hand1_9_left_seg_4_cropped.jpeg' \n",
    "img=image.load_img(fl_img,target_size=(64,64))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x,axis=0)\n",
    "pred=np.argmax(new_model.predict(x))\n",
    "print(pred)\n",
    "op=['0','1','2','3','4','5','6','7','8','9','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "op[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694ca45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
